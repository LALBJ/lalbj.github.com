<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG" />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta
      name="twitter:image"
      content="static/images/your_twitter_banner_image.png"
    />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      Paying More Attention to Image: A Training-Free Method for Alleviating
      Hallucination in LVLMs
    </title>
    <link rel="icon" type="image/x-icon" href="static/images/PAI.webp" />
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                <img width="40px" src="static/images/PAI.webp" alt="logo" />
                <span class="highlight">P</span>aying More
                <span class="highlight">A</span>ttention to
                <span class="highlight">I</span>mage: A Training-Free Method for
                Alleviating Hallucination in LVLMs
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <a href="FIRST AUTHOR PERSONAL LINK" target="_blank"
                    >Shi Liu</a
                  >,</span
                >
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank"
                    >Kecheng Zheng</a
                  >,</span
                >
                <span class="author-block">
                  <a href="THIRD AUTHOR PERSONAL LINK" target="_blank"
                    >Wei Chen</a
                  >
                </span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"
                  >State Key Lab of CAD&CG, Zhejiang University<br />ECCV
                  2024</span
                >
                <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/YOUR REPO HERE"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/<ARXIV PAPER ID>"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Teaser video-->
    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/teaser_v3.jpg" alt="teaser" />
          <h2 class="subtitle has-text-centered">
            We discovered the
            <span class="soft-highlight">"Text Inertia"</span> phenomenon in
            LVLMs, which means
            <span class="soft-highlight"
              >some hallucinations are caused by text context instead of
              misinterpretation of image information</span
            >. We proposed a method called PAI to alleviate this phenomenon.
          </h2>
        </div>
      </div>
    </section>
    <!-- End teaser video -->

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Large Vision-Language Models (LVLMs) align image features to the
                input of Large Language Models (LLMs), enhancing multi-modal
                reasoning and knowledge utilization capabilities. However, the
                disparity in scale between models of different modalities has
                resulted in LLMs assuming a predominant role in multimodal
                comprehension. This imbalance in model integration can lead to
                instances of hallucinatory outputs.
                <span class="soft-highlight">
                  In particular, LVLMs may generate descriptions that persist in
                  the absence of visual input, suggesting that these narratives
                  are disproportionately influenced by the textual context. We
                  refer to this phenomenon as ``text inertia.''
                </span>
                To counteract this issue, we introduce
                <span class="soft-highlight">a training-free algorithm</span>
                designed to find an equilibrium between image comprehension and
                language inference. Specifically, we
                <span class="soft-highlight"
                  >involve adjusting and amplifying the attention weights
                  assigned to image tokens</span
                >, thereby granting greater prominence to visual elements.
                Meanwhile, we
                <span class="soft-highlight"
                  >the logits of multimodal inputs from the model logits of pure
                  text input</span
                >, which can let model not biased towards only LLM. By enhancing
                images tokens and reducing the stubborn output of LLM, we can
                let LVLM pay more attention to images, towards alleviating text
                inertia and reducing the hallucination in LVLMs. Our extensive
                experiments shows that this method substantially reduces the
                frequency of hallucinatory outputs in various LVLMs in terms of
                different metrics.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->

    <!-- Image carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Method Overview</h2>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <img src="static/images/framework_v2.png" alt="teaser" />
              <h2 class="subtitle has-text-centered">
                Our method primarily consists of two components: (1)
                <span class="soft-highlight">inference intervention</span>:
                scaling up the attention weights assigned to image tokens, and
                (2) <span class="soft-highlight">logits refinement</span>:
                mitigating the influence of pure text context by subtracting
                pure text logits.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Image carousel -->
    <section class="hero is-small is-light">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Main Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item my-item">
              <!-- Your image here -->
              <img src="static/images/CHAIR.png" alt="CHAIR Results" />
              <h2 class="subtitle has-text-centered">
                Evaluation Results on CHAIR: which evaluates hallucinations in a
                long sequence generation paradigm.
              </h2>
            </div>
            <div class="item my-item">
              <!-- Your image here -->
              <img src="static/images/POPE.png" alt="POPE Results" />
              <h2 class="subtitle has-text-centered">
                Evaluation Results on POPE: which evaluates hallucinations in a
                VQA paradigm.
                <span class="soft-highlight"
                  >We constructed the evaluation into single-turn and multi-turn
                  categories.</span
                >
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!-- Image carousel -->
    <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">More Show Cases</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item my-item">
              <!-- Your image here -->
              <img src="static/images/llava_qual_case.png" alt="CHAIR Results" />
              <h2 class="subtitle has-text-centered">
                PAI's performance on reducing hallucinations of LLaVA-1.5-7B.
              </h2>
            </div>
            <div class="item my-item">
              <!-- Your image here -->
              <img src="static/images/minigpt4_qual_case.png" alt="POPE Results" />
              <h2 class="subtitle has-text-centered">
                PAI's performance on reducing hallucinations of Minigpt4.
              </h2>
            </div>
            <div class="item my-item">
              <!-- Your image here -->
              <img src="static/images/shikra_qual_case.png" alt="POPE Results" />
              <h2 class="subtitle has-text-centered">
                PAI's performance on reducing hallucinations of Shikra.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End image carousel -->

    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>BibTex Code Here</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page. You are free to borrow the of this website, we
                just ask that you link back to this page in the footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
