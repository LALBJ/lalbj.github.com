---
title: 一问读懂 L1 正则化(Lasso)、L2 正则化(Ridge)、权重衰减
date: 2022-10-16 18:31:52
tags:
- 涨知识
categories:
- [机器学习]
- [正则化]
toc: true
---
正则化
在训练模型的过程中经常会遇到两个问题：过拟合和欠拟合
正则化技术则是用于防止过拟合的一大利器。
那么正则化技术是怎么样使得模型不倾向于过拟合的呢？
通过缩减系数项使得模型对于自变量的敏感度降低，进一步降低模型的整体方差。
<!-- more -->
在实践中主要的正则化技术包括：
L2 正则化
L1 正则化
数据增强
Dropout
Early Stopping
本文中主要介绍， L1 与 L2 正则化，其中他们二者分别也可以被称为 lasso 回归和 ridge 回归。
lasso 回归和 ridge 回归有哪些区别？为什么 lasso 回归可以用于特征选择而后者不能？L2 正则化和权重衰减又有这哪些细微差别？
这些问题看完本文后都能够告诉你答案！
L1 正则化 (Lasso) 和 L2 正则化 (Ridge)
在介绍 L1 正则化和 L2 正则化之前，先对于范数概念进行介绍。
在数学上，针对于 p-norm 的严格定义是：
$$ ||x||p:=(∑i=1n|xi|p)(1/p) $$
其中，当 p 取 1 时被称为 1-norm，也就是提到的 L1-norm 同理 L2-norm。
而我们的 L1 正则化与 L2 正则化实际上也就是在损失函数中加入 L1/L2 范数项，并且使用 λ 作为惩罚系数，防止过拟合。
二者的公式如下：
$$ Lasso(w^)=∑i=1n(yi−xiw^)2+λ∑j=1m|wi| $$
 
关于这两个正则项的主要的不同，包括以下几点：
L2 计算起来更方便，而 L1 特别是在非稀疏向量上的计算效率就很低；
L1 正则项的输出稀疏，会把不重要的特征直接置0，而 L2 不会；
L2 有唯一解，而 L1 不是。
关于计算方便与唯一解，是因为 L2 为两次项，求最小值则可以直接通过求导后计算最小值即可。但是 L1 为一个绝对值项，对于一个绝对值项去计算最小值是非常麻烦的。
关于输出稀疏这一点，也是 L1 正则项能够被用于特征选择的原因。
这里简要的介绍一下，因为 L1 不管大小为多少梯度都为 1 或者 -1，所以每次更新都是稳步向 0 前进。而反观 L2 就会发现，它的梯度越靠近 0，就会变得越小。
也就是说 L1 正则经过一定步数很可能变为0，而 L2 则不太可能，因为在其值小的时候梯度也会变小。这就造就了 L1 的输出稀疏的特性。
正则化的原理
首先，我们推导一下 L2 正则化产生作用的原理。
针对损失函数：
 
我们对  求偏导数得到
 
基于反向传播算法的  的学习规则变为：
 
通过上式可以看出，正则化过程中，会使得网络的权重更小。而权重更小意味着，如果我们在这里或那里改变一些随机的输入，网络的行为不会有太大的变化，这反过来使正则化很难学习到数据中的局部噪声。这也是 L2 正则化被称为权重衰减的原因，因为它是权重减小。
权重衰减与 L2 正则化
L2 正则化和权重衰减并不是一回事，但是在 SGD 中两个方法是等价的。
以  为衰减因子，给出权值的衰减方程
 
与上面 L2 正则化的式子进行对比，可以看出在SGD中将 L2 正则化的更新方程重新参数化后（  ）二者之间是等价的
结语
在最后，我们将上文讲述的内容进行总结：
L1 正则化与 L2 正则化的损失函数是及其相似的，二者使用的 范数项不同
L1 正则化具有输出稀疏的特点，并且能够用于特征选择；L2 正则化的计算较快
权重衰减和 L2 正则化概念上比较相似，但是不能够完全等价
Referrence
Lasso and Ridge Regression Tutorial
权值衰减和L2正则化傻傻分不清楚？
l1正则与l2正则的特点是什么，各有什么优势？