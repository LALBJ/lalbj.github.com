---
title: Convolutional Neural Networks for Sentence Classification(TextCNN)
date: 2023-01-02 16:43:36
tags:
---

## 摘要

我们报告了一系列的卷积神经网络实验（CNN），这些实验室都是基于预训练的词向量做句子级别的分类预测任务。我们展示了一个简单的 CNN 基于很少的超参数和静态向量在多个 benchmark 上取得了很好的结果。通过微调学习具体任务的向量表示能够得到更好的性能。我们对架构进行了简单的修改使其能够支持具体任务的向量表示和静态向量表示。我们通过 CNN 模型在 4 个任务上取得了 sota （一共测试了 7 个任务），其中包括情感分析和问题分类。

<!--more-->

## 引言

在这篇文章发表之前 CNN 模型就已经在图像领域和语音识别领域取得了非常好的成效。在过去的一些 NLP 的工作也有一些使用了深度学习的方法，用于将词向量的表示（将 V 个单词通过隐藏层编码到低维空间，其中 V 为词表大小，而隐藏层本质上是对单词进行特征提取）。通过这样的表示，语义接近的单词更可能在低维的向量空间中有更近的距离表示可以是欧几里得距离也可以是余弦距离。

在第二段作者又对 CNN 进行了介绍，结构非常清晰。在此篇工作之前，CNN 主要用于图像处理、语音识别以及一些传统的 NLP 任务。

第三段中，作者对于自己的工作进行了简要介绍：我们对于 Google News 数据集的单词使用了一个简单的 CNN 结构去得到他们的词向量。我们首先使用这些静态的词向量只对模型的一些超参数进行微调，我们的结果表明这种结构只需要很少的超参数微调操作就能够在多个 benchmark 上取得很好的结果。这个结果表明我们的 CNN 结构词向量表示方法能够进行通用特征抽取，这些特征能够用于多种不同的分类任务。而通过基于结果进行微调学习具体任务的向量表示能够进一步提升效果。所以我们最后对我们的模型结构进行了简单的更新是他能够同时使用预训练的词向量和具体任务的词向量表示。

## 2 模型

我们使用 $x_i\in \mathbb{R}^k$ 表示句子中第 i 个单词的 k 维向量。长度为 n 的句子（因为句子长度可变，所以q需要使用填充）表示为：
$$
\begin{equation}
x_{1:n} = x_1 \oplus x_2 \oplus ... \oplus x_n,
\end{equation}
$$

其中 $\oplus$ 表示连结操作符。$x_{i:i+j}$ 表示单词$x_i, x_i+1, ..., x_{i+j}$的连接。卷积操作涉及到一个滤波器 $w \in \mathbb{R}^{hk}$，这个滤波器用于对窗口大小 h 内的单词构建新的特征。比如，一个特征 $c_i$ 由单词窗口 $x_{i:i+h-1}$ 生成：

$$
\begin{equation}
c_i = f(w \cdot x_{i:i+h-1} + b).
\end{equation}
$$

$b \in \mathbb{R}$ 是一个偏置项而 $f$ 是一个非线性的激活函数（文中作者使用的是双向正切函数，tanh）。这个滤波器将对句子中所有窗口${x_{1:h}, x_{2:h+1}, ..., x_{n-h+1}}$生成一个特征映射：
$$
\begin{equation}
\bold{c} = [c_1, c_2, ..., c_n-h+1],
\end{equation}
$$

其中 $c \in \mathbb{R}^{n-h+1}$。我们接下来使用一个最大池化操作基于特征映射选取最大值 $\hat{c} = max{\bold{c}}$。这个操作的目的是捕获到最重要的特征。这种池化模式能够很自然地用于处理可变的句子长度。

上面介绍了单滤波器构建单特征的过程。实际上，这个模型使用了多个滤波器（不同的窗口大小）构建了多个特征。这些特征构成了倒数第二层，最后一层是一个全连接层加上 softmax 输出每个类别的概率。

作为模型的一个变体，我们实验了双通道的词向量——一个是训练好的静态向量和一个通过反向传播微调的向量（3.2 章节中会详细介绍）。多通道架构，如图1所示，每个通道共用一套滤波器。

![Figure 1:双通道样例结构](/images/TextCNN/fig1.png)

### 2.1 正则化

这里作者主要介绍了 dropout 的用法和原理，dropout 现在已经相当成熟了这里就不过多介绍了。

## 3 数据集和实验配置

作者使用的 benchmark 在表 1 中进行了总结

![Table 1:令牌化后的数据集统计数据](/images/TextCNN/table1.png)

### 3.1 超参数和训练

一些常规的超参数和实验数据集配置的介绍，这里不过多介绍了。

### 3.2 预训练的词向量

这里其实就是使用了 word2vec 基于 Google News 的预训练结果

### 3.3 模型变种

有点类似消融实验，这里作者给出了四种模型变种：

- CNN-rand：baseline 模型，所有的词向量是随机初始化的再通过训练进行更新

- CNN-static：基于 word2vec 预训练词向量的模型。所有的单词不在训练过程中修改，训练过程中只更新模型学习到的参数

- CNN-non-static：与上一个模型相同，但是预训练向量会基于每一个任务微调

- CNN-multichannel：即图 1 介绍的模型结构

## 实验结果和讨论

实验结果如表 2 所示
![](/images/TextCNN/table1.png)

